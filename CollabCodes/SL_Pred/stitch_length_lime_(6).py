# -*- coding: utf-8 -*-
"""Stitch_Length_Lime_(6).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QrxZM-ThDfcb929GSw8409yx6FXWh93j

Regression DNN
"""

!pip install -q -U keras-tuner

from pandas import read_csv

from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from keras.layers import Dropout 
from keras import regularizers 

import keras_tuner

from tensorflow import keras

# load data and arrange into Pandas dataframe
df = read_csv("dataset_prepro.csv")

print(df.head())

print(df.describe())

#Split into features and target (Price)
X = df.drop('stitch_length', axis = 1)
y = df['stitch_length']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 20)

#Scale dataset
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = Sequential()
model.add(Dense(50, input_dim=16, activation='tanh'))
model.add(Dense(80, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(1))

import tensorflow as tf
tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model.compile(loss='mean_squared_error', optimizer='Adam', metrics=['mean_absolute_percentage_error'])

from keras import callbacks
earlystopping = callbacks.EarlyStopping(monitor ="val_loss", 
                                        mode ="min", patience = 5, 
                                        restore_best_weights = True)
  
history = model.fit(X_train_scaled, y_train, batch_size = 128, 
                    epochs = 100, validation_split=0.2, 
                    callbacks =[earlystopping])

#fitting Model
#history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs =100, callbacks =[earlystopping])

from matplotlib import pyplot as plt
#plot the training and validation accuracy and loss at each epoch
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#Predict on test data
predictions = model.predict(X_test_scaled[:10])
y_real=y_test[:10]
print("Predicted values are: ", predictions)
print("Real values are: ", y_real)

from sklearn.metrics import r2_score
r2=r2_score(y_real,predictions)
r2

!pip install lime

import lime
import lime.lime_tabular

feature_names = ['count', 'density', 'width','white', 'light', 'medium', 'dark', 'extra_dark', 'black', 'diameter', 'gauge', 'needles', 'feeders', 'rpm', 'shrinkage_length', 'shrinkage_width','stitch_length']

explainer = lime.lime_tabular.LimeTabularExplainer(X_test_scaled, feature_names=feature_names, verbose=True, mode='regression')

import pandas as pd
feature_names = ['count', 'density', 'width','white', 'light', 'medium', 'dark', 'extra_dark', 'black', 'diameter', 'gauge', 'needles', 'feeders', 'rpm', 'shrinkage_length', 'shrinkage_width']

#import numpy as np
#a=X_test_scaled[:4]
#a

df=pd.DataFrame(a)
a=df.iloc[1]

explain_data_point = explainer.explain_instance(a,model.predict, num_features=16)
explain_data_point.as_pyplot_figure();